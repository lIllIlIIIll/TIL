## ML 방법론

  - Supervision : 입력 데이터에 대해 어떤 결과가 나와야하는지 사람이 직접 지정

  - 지도학습 : 입력 데이터에서 함수의 출력값이 라벨에 가까워지도록 학습

      - 회귀 모델 : 모델의 종속변수가 연속형 변수인 모델

        - 선형회귀모델
       
        - 상관관계 분석
   
      - 분류 모델 : 모델의 종속변수가 범주형 또는 이산형 변수인 모델

        - 로지스틱 회귀모델(이진 분류)
       
        - k-최근접 이웃

        - 결정 트리
       
        - 서포트 벡터머신

  - 비지도학습 : 레이블을 활용하지 않고 입력 데이터로만 학습

      - 매니폴드 가설
   
      - 차원축소 : PCA, t-SNE
   
      - 군집화 : k-Means
   
## 지도학습
   
### 회귀모델

  - 입력변수에 따라서...

    - 한 개 : 단순 회귀분석
   
    - 여러개 : 다중 회귀분석
   
  - 출력변수에 따라서...

    - 한 개 : (단변량) 회귀분석
   
    - 여러개 : 다변량 회귀분석
   
  - 사용하려는 모델이 선형인지에 따라서...

    - 선형 회귀분석
   
    - 비선형 회귀분석
   
  - 선형 회귀모델 : 직선의 기울기와 절편을 조절해 데이터의 오차가 가장 작아지도록 조정

    - MSE 손실함수를 사용
   
    - 수치적 해법 : 점진적으로, 근사적인 해를 찾는 방법, 일반적으로 많은 경우에 사용 가능
   
    - 해석적 해법 : 식 전개, 특정 규칙을 통해 해를 찾는 방법, 사용 조건이 까다롭거나, 아예 쓸 수 없는 경우 존재
   
    - 상관관계 : 한 변수가 변화할 때 다른 변수가 함께 변화하는 경향성을 보임
   
    - 인과관계 : 한 변수의 변화가 원인이 되어 그 결과로서 다른 변수를 변화시킴

### 분류모델

  - 이진 분류 : 출력값이 참 or 거짓 두가지로만 나오는 간단한 형태의 분류 모델

    - 로지스틱 함수 : 인풋을 0과 1사이로 변환해 확률로 해석될 수 있는 결과로 내보내주는 함수
   
    - 시그모이드 함수 : 로지스틱 함수를 포함하여 S자 모양으로 함수 형태가 나오는 함수들의 총칭
   
    - 크로스 엔트로피

      - 로지스틱 함수를 통과한 모델의 출력값 확률에 대해 적용
     
      - BCE : 이진 크로스 엔트로피
     
  - 다중 분류 : 클래스가 k개인 분류 문제

    - 원핫 인코딩 : 정답 클래스가 i번째일 경우 i번째 원소만 1, 나머지는 모두 0인 벡터로 라벨 표현
   
    - 소프트맥스 : 다중분류 모델에서 결과 logit 벡터를 각각 클래스에 대한 확률을 나타내는 벡터로 변환
   
  - kNN 알고리즘 : 특정 인풋이 들어왔을 때, 학습 데이터에서 가장 근접한 k개 라벨을 기준으로 출력값을 결정

  - 결정트리 : 독립변수 내 대소 관계 or 특정 임계값과 비교등의 판단을 계층적으로 적용하여 최종 결과 분류

    - 불순도 : 한 범주 안에 데이터가 얼마나 섞여 있는지 측정 → 손실함수와 같은 역할
   
      - 엔트로피, 지니계수 등등..
      
  - 랜덤 포레스트 : 랜덤 구조의 결정트리를 여러개 생성한 뒤 각 트리의 결과를 종합하여 최종 출력값을 결정 → 앙상블

  - 서포트 벡터머신 : 이진분류 모델에서 판단의 기준이 되는 초평면을 설정(결정경계)하여 이를 기준으로 양쪽이 서로 다른 클래스로 분류

      - 커널 트릭 : 커널함수를 통해 데이터를 고차원으로 변환하여 선형분리가 가능해지도록 만듦
   
## 비지도학습

  - 군집화 : 데이터로부터 패턴을 파악하여 데이터를 여러 군집으로 나누는 것

  - 군집화 VS 분류

    - 분류 : 주어진 특성을 입력값으로 사용하여 클래스를 추론, 모델의 결과와 **레이블**을 비교하여 학습
   
    - 군집화 : 주어진 샘플들을 어떤식으로 나누어야 가장 유사한 샘플끼리 묶일지 **데이터 간 관계**를 비교하여 학습
   
  - 매니폴드 가설 : 일반적인 고차원 데이터들은 가능한 모든 데이터 공간에 고르게 분포하는 것이 아니라 상대적으로 매우 적은 차원의 매니폴드를 이루고 있을 것이다.

  - 차원축소 : PCA(주성분분석)

  - 시각화 : t-SNE

  - 이상값 탐지 : 매니폴드 구조를 학습하고 거기에 벗어나있는 데이터 포인트를 탐색

  - 단어 임베딩 : 자연어 단어들을 의미적인 연산이 가능한 임베딩 공간에 맵핑

  - 생성모델 : 저차원 임베딩 공간 → 원래의 이미지 공간으로 가는 함수 학습, 이를 통해 새로운 특성을 가진 샘플 생성

### 고차원 데이터

  - 데이터 내 특성이 많아 차원이 높은 데이터

  - 고차원에서 생기는 문제점

    - 컴퓨터 자원 사용량
   
    - 차원에 따른 데이터 희소 문제
   
    - 직관적으로 이해하기 힘든 고차원 공간
   
## 회고록

지도학습 부분은 많은 경진대회에서 사용되는 모델 학습 방법론이기에 쉽게 이해할 수 있었다.

그러나 비지도학습으로 넘어와서 PCA 부분을 학습할 때, 수학과 연관지어 설명하니까 난이도가 확 올라가는것이 느껴졌다.

SVD를 이용하여 직접 차원축소를 하는 과정을 보여줄 때 굉장히 난해했지만 여러번 이해할 때까지 반복하여 보았다.

그냥 PCA 모듈을 불러와서 사용할 때는 몰랐으나 이런식으로 축소되는 전체적인 과정을 파악하니 어려웠다.

CNN과 같은 이미지 모델 학습은 내가 많이 부족한 부분이기도 하기에, 차원축소 부분은 반드시 복습하자.
